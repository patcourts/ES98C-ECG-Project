{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/549 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering Database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 549/549 [01:01<00:00,  8.93it/s]\n",
      "  5%|████▎                                                                           | 12/221 [00:00<00:01, 118.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221 remaining out of 290\n",
      "normalising and preproccessing signals using DWT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 221/221 [00:01<00:00, 134.65it/s]\n"
     ]
    }
   ],
   "source": [
    "%run \"Data_PreProcessing.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Myocardial infarction': 111, 'Healthy control': 44, 'Dysrhythmia': 9, 'Cardiomyopathy': 10, 'Hypertrophy': 7, 'Bundle branch block': 10, 'Valvular heart disease': 1, 'Stable angina': 1, 'Myocarditis': 3}\n"
     ]
    }
   ],
   "source": [
    "#using one channel only for now\n",
    "\n",
    "def get_diagnosis_counts(labels, diagnosis_set):\n",
    "    diagnosis_counts = {}\n",
    "    labels = list(labels)\n",
    "    for diagnosis in diagnosis_set:\n",
    "        diag_count = labels.count(diagnosis)\n",
    "        diagnosis_counts[diagnosis] = diag_count\n",
    "    return diagnosis_counts\n",
    "\n",
    "\n",
    "#getting labels for patient set\n",
    "detailed_health_state = []\n",
    "for i in range(0, no_patients):\n",
    "    detailed_health_state.append(allowed_patients.get_patients(i).get_diagnosis())\n",
    "\n",
    "detailed_health_state = np.array(detailed_health_state)\n",
    "\n",
    "#getting nan indices\n",
    "nan_indices = []\n",
    "for j in range(0, no_channels):\n",
    "    signal_nan_indices = []\n",
    "    for i, signal in enumerate(denoised_signals[:, j]):\n",
    "        if np.isnan(signal).all():\n",
    "            signal_nan_indices.append(False)\n",
    "        else:\n",
    "            signal_nan_indices.append(True)\n",
    "    nan_indices.append(signal_nan_indices)\n",
    "\n",
    "nan_indices = np.array(nan_indices)\n",
    "\n",
    "diag_counts = get_diagnosis_counts(detailed_health_state[nan_indices[0]], allowed_patients.get_diagnosis_list())\n",
    "print(diag_counts)\n",
    "\n",
    "#these have very few appearances in data set so are removed\n",
    "diagnoses_to_remove = ['Stable angina', 'Myocarditis', 'Valvular heart disease', 'Hypertrophy']\n",
    "\n",
    "#finding indices with which to remove from the dataset\n",
    "diagnosis_indices = []\n",
    "health_state = []\n",
    "for j in range(0, no_channels):\n",
    "    filter_diagnosis = []\n",
    "    for diagnosis in detailed_health_state[nan_indices[j]]:\n",
    "        if diagnosis in diagnoses_to_remove:\n",
    "            filter_diagnosis.append(False)\n",
    "        else:\n",
    "            filter_diagnosis.append(True)\n",
    "    diagnosis_indices.append(filter_diagnosis)\n",
    "    health_state.append(detailed_health_state[nan_indices[j]][diagnosis_indices[j]])\n",
    "\n",
    "data = denoised_signals[:, 0][nan_indices[0]][diagnosis_indices[0]]\n",
    "labels = health_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TypeAlias' from 'typing_extensions' (C:\\Users\\court\\Documents\\anaconda\\lib\\site-packages\\typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-815ac2e4f1ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   1454\u001b[0m \u001b[1;31m# If you edit these imports, please update torch/__init__.py.in as well\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1455\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_rng_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_rng_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmanual_seed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_seed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1456\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mserialization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1457\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_tensor_str\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_printoptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_get_dtype_from_pickle_storage_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBinaryIO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTypeAlias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeGuard\u001b[0m  \u001b[1;31m# Python 3.10+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopyreg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TypeAlias' from 'typing_extensions' (C:\\Users\\court\\Documents\\anaconda\\lib\\site-packages\\typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Define a simple neural network as the embedding function\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define the Prototypical Network class\n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        self.encoder = EmbeddingNet(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def compute_prototypes(self, support_data, support_labels, n_classes):\n",
    "        prototypes = []\n",
    "        for i in range(n_classes):\n",
    "            class_samples = support_data[support_labels == i]\n",
    "            class_prototype = class_samples.mean(dim=0)\n",
    "            prototypes.append(class_prototype)\n",
    "        return torch.stack(prototypes)\n",
    "\n",
    "    def classify(self, prototypes, query_data):\n",
    "        distances = torch.cdist(query_data, prototypes)\n",
    "        return torch.argmin(distances, dim=1)\n",
    "\n",
    "# Define a function to train the prototypical network\n",
    "def train_prototypical_network(model, train_loader, optimizer, criterion, n_classes):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        support_data, support_labels, query_data, query_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute embeddings for support and query data\n",
    "        support_embeddings = model(support_data)\n",
    "        query_embeddings = model(query_data)\n",
    "\n",
    "        # Compute prototypes\n",
    "        prototypes = model.compute_prototypes(support_embeddings, support_labels, n_classes)\n",
    "\n",
    "        # Classify query samples\n",
    "        predictions = model.classify(prototypes, query_embeddings)\n",
    "        loss = criterion(predictions, query_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Define a function to evaluate the prototypical network\n",
    "def evaluate_prototypical_network(model, test_loader, n_classes):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            support_data, support_labels, query_data, query_labels = batch\n",
    "\n",
    "            # Compute embeddings for support and query data\n",
    "            support_embeddings = model(support_data)\n",
    "            query_embeddings = model(query_data)\n",
    "\n",
    "            # Compute prototypes\n",
    "            prototypes = model.compute_prototypes(support_embeddings, support_labels, n_classes)\n",
    "\n",
    "            # Classify query samples\n",
    "            predictions = model.classify(prototypes, query_embeddings)\n",
    "            correct += (predictions == query_labels).sum().item()\n",
    "            total += query_labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "class PTBDataSet(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Parameters\n",
    "input_dim = 10\n",
    "hidden_dim = 64\n",
    "output_dim = 32\n",
    "n_classes = 5\n",
    "n_support = 5\n",
    "n_query = 15\n",
    "learning_rate = 0.001\n",
    "n_epochs = 100\n",
    "\n",
    "\n",
    "# Split data into support and query sets\n",
    "support_data = torch.tensor(data[:n_support*n_classes], dtype=torch.float32)\n",
    "support_labels = torch.tensor(labels[:n_support*n_classes], dtype=torch.long)\n",
    "query_data = torch.tensor(data[n_support*n_classes:], dtype=torch.float32)\n",
    "query_labels = torch.tensor(labels[n_support*n_classes:], dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(PTBDataset(support_data, support_labels), batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(PTBDataset(query_data, query_labels), batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = PrototypicalNetwork(input_dim, hidden_dim, output_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate the model\n",
    "for epoch in range(n_epochs):\n",
    "    train_prototypical_network(model, train_loader, optimizer, criterion, n_classes)\n",
    "    accuracy = evaluate_prototypical_network(model, test_loader, n_classes)\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, Accuracy: {accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.1-cp38-cp38-win_amd64.whl (1.9 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow-intel==2.13.1; platform_system == \"Windows\" (from tensorflow) (from versions: 0.0.1, 2.10.0.dev20220728, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0)\n",
      "ERROR: No matching distribution found for tensorflow-intel==2.13.1; platform_system == \"Windows\" (from tensorflow)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jmlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0e3bb32b4dd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mjmlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjmlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloaders\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPTBXL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjmlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfiltering\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mButterworth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjmlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLambdaModule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjmlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWriter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jmlib'"
     ]
    }
   ],
   "source": [
    "from jmlib.data import Data\n",
    "from jmlib.data.loaders import PTBXL\n",
    "\n",
    "from jmlib.data.processing.common import LambdaModule\n",
    "from jmlib.data.writers.common import Writer\n",
    "from jmlib.data.generators.fewshot import FewShotGenerator\n",
    "from jmlib.data.splitters.common import ClassSplitter \n",
    "\n",
    "from typing import Unpack\n",
    "from keras.layers import TimeDistributed, Lambda, Activation, Flatten, Dense\n",
    "from keras.layers import BatchNormalization, MaxPooling1D, Conv1D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "\n",
    "from jmlib.models.common import BaseModel, BaseModelParams\n",
    "from jmlib.util.models import reduce_tensor, reshape_query, proto_dist\n",
    "from jmlib.util.models import LinearFusion\n",
    "\n",
    "from keras.metrics import AUC\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ptbxl = Data(name=\"raw_PTBXL\", verbose=True)\n",
    "ptbxl.add(\n",
    "    PTBXL(data_dir=\"/Users/jbthompson/Documents/final_folder/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\"),\n",
    "    ClassSplitter({\"train\": 0.6, \"val\": 0.2, \"test\": 0.2}),\n",
    "    FewShotGenerator(way=5, shot=5, query= 5, batch_size=100)\n",
    ")\n",
    "ptbxl.run()\n",
    "\n",
    "KERNEL_TUPLE = tuple[tuple, tuple]\n",
    "EPOCHS = 100\n",
    "\n",
    "class PMCNN2022(BaseModel):\n",
    "    \"\"\"Zicong Li et al., 2022 - https://doi.org/10.1109/BHI56158.2022.9926948.\n",
    "\n",
    "    Implementation by Joe McMahon. Adapted from from code provided by\n",
    "    Zicong Li. Summary from paper: \"a parallel multi-scale CNN (PM-CNN) based\n",
    "    prototypical network for arrhythmia classification\".\n",
    "    This implementation supports categorical classification.\n",
    "    Designed for use on the CPSC-2018 dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float, default=0.001\n",
    "        Learning rate.\n",
    "    depth : int, default=4\n",
    "        Number of convolutional layers in the prototypical network.\n",
    "    fd : int, default=512\n",
    "        The shape of the 1D feature vector output of the fusion layer.\n",
    "    kernels : tuple of tuples of ints, default=((3,3),(7,7))\n",
    "        Convolutional kernel shapes.\n",
    "    filters : int, default=64\n",
    "        Number of filters in convolutional layers.\n",
    "    **kwargs\n",
    "        keyword arguments to pass to super class. See jmlib.models.BaseClass.\n",
    "    \"\"\"\n",
    "\n",
    "    LR: float\n",
    "    DEPTH: int\n",
    "    FD: int\n",
    "    KERNELS: KERNEL_TUPLE\n",
    "    FILTERS: int\n",
    "\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.001,\n",
    "                 depth: int = 4,\n",
    "                 fd: int = 512,\n",
    "                 kernels: KERNEL_TUPLE = (3, 7),\n",
    "                 filters: int = 64,\n",
    "                 **kwargs: Unpack[BaseModelParams]):\n",
    "        self.LR = lr\n",
    "        self.DEPTH = depth\n",
    "        self.FD = fd\n",
    "        self.KERNELS = kernels\n",
    "        self.FILTERS = filters\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _layers(self, X):\n",
    "        Xs, Xq = X\n",
    "        shot = Xs.shape[-4]\n",
    "        query = Xq.shape[-4]\n",
    "\n",
    "        proto_model3 = TimeDistributed(\n",
    "            self._proto_model(self.KERNELS[0]), name=\"Prototype_CNN_3\"\n",
    "        )\n",
    "        proto_model7 = TimeDistributed(\n",
    "            self._proto_model(self.KERNELS[1]), name=\"Prototype_CNN_7\"\n",
    "        )\n",
    "\n",
    "        Xs3 = proto_model3(Xs)\n",
    "        Xq3 = proto_model3(Xq)\n",
    "\n",
    "        Xs7 = proto_model7(Xs)\n",
    "        Xq7 = proto_model7(Xq)\n",
    "\n",
    "        Xs = LinearFusion(shot, self.FD)(Xs7, Xs3)\n",
    "        Xq = LinearFusion(query, self.FD)(Xq7, Xq3)\n",
    "\n",
    "        Xs = Lambda(reduce_tensor, name=\"Reduce_Support\")(Xs)\n",
    "        Xq = Lambda(reshape_query, name=\"Reshape_Query\")(Xq)\n",
    "\n",
    "        X = Lambda(proto_dist, name=\"Prototype_Distance\")([Xs, Xq])\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _proto_model(self, k) -> Sequential:\n",
    "        cnn = Sequential()\n",
    "        for _ in range(self.DEPTH):  # type: ignore\n",
    "            cnn.add(Conv1D(self.FILTERS, k, padding='same'))\n",
    "            cnn.add(BatchNormalization())\n",
    "            cnn.add(Activation('relu'))\n",
    "            cnn.add(MaxPooling1D())\n",
    "\n",
    "        cnn.add(Flatten())\n",
    "        cnn.add(Dense(self.FD))\n",
    "        return cnn\n",
    "\n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        \"\"\"Adam Optimizer.\"\"\"\n",
    "        return Adam(learning_rate=self.LR)  # type: ignore\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        \"\"\"Categorical Crossentropy Loss.\"\"\"\n",
    "        return CategoricalCrossentropy()\n",
    "\n",
    "    @property\n",
    "    def callbacks(self):\n",
    "        \"\"\"Callbacks.\"\"\"\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.4,\n",
    "            patience=2,\n",
    "            min_lr=1e-8,  # type: ignore\n",
    "            cooldown=2\n",
    "        )\n",
    "        return [reduce_lr]\n",
    "    \n",
    "mymodel = PMCNN2022(input_tensors=ptbxl.tensor)\n",
    "mymodel.model.compile(optimizer=mymodel.optimizer, loss=mymodel.loss, metrics=['categorical_accuracy'])\n",
    "mymodel.model.summary()\n",
    "results = mymodel.model.fit(ptbxl.generators['train'], epochs=EPOCHS, validation_data=ptbxl.generators['val'], verbose=2)\n",
    "test_results = mymodel.model.evaluate(ptbxl.generators['test'], verbose=2, return_dict=True)\n",
    "print(test_results)\n",
    "\n",
    "\n",
    "x_data = np.arange(1, EPOCHS + 1)\n",
    "plt.figure()\n",
    "plt.plot(x_data, results.history['categorical_accuracy'], label='Training Accuracy')\n",
    "plt.plot(x_data, results.history['val_categorical_accuracy'],label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_data, results.history['loss'], label='Training Loss')\n",
    "plt.plot(x_data, results.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "repeated_array = np.tile(ptbxl.generators['test']['data'][1], (100, 1))\n",
    "true_labels_categorical = np.argmax(repeated_array, axis=1)\n",
    "\n",
    "# Assuming `test_results` is a dictionary containing predictions and true labels\n",
    "predictions = mymodel.model.predict(ptbxl.generators['test'])\n",
    "true_labels = true_labels_categorical\n",
    "\n",
    "# Assuming your predictions are probabilities and you want to convert them to class labels\n",
    "predicted_labels = predictions.argmax(axis=1)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(true_labels, predictions, multi_class='ovr')\n",
    "\n",
    "print(\"Overall F1 Score:\", f1)\n",
    "print('Overall AUC Score:', auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
