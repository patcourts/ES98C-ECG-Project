{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes access to detrended, denoised signals and calculates various parameters from these signals. The parameters estimated within this notebook are split into three broad categories: time domain, frequency-domain and non-linear domain. They have been chosen due to their successes in literature relating to ECG classification as well as their use as Heart rate variability (HRV) metrics:\n",
    "\n",
    "Time domain:\n",
    " - mean RR intervals\n",
    " - std RR intervals \n",
    " - mean amplitude\n",
    " - RMSSD of differences between successive RR intervals \n",
    " - pNN50 (%) NN50 divided by total number of RR (NN50 is the number of sucessive RR intervals that differ by more than 50ms) \n",
    " - mean QRS duration\n",
    " - std QRS duration\n",
    " - moments of the signal (physiological measures)\n",
    " \n",
    " \n",
    "Frequency Domain:\n",
    "- absolute power of LF (0.04 - 0.15 Hz) band\n",
    "- absolute power of HF (0.15 to 0.4Hz) band\n",
    "- LF/HF ratio\n",
    "- Total power (0-0.4 Hz)\n",
    "\n",
    "Non-linear:\n",
    "- Fractal dimension of dynamic attractor of signal\n",
    "- std of poincare\n",
    "- Shannon entropy\n",
    "- sample entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering Database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549/549 [01:00<00:00,  9.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221 remaining out of 290\n",
      "normalising and preproccessing signals using DWT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [00:01<00:00, 129.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "%run \"Data_PreProcessing.ipynb\" #allowing access to the filtered database with preprocessed signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import correlate #for autocorrelation for signal embedding\n",
    "from scipy.stats import skew, kurtosis #for signal measures\n",
    "from scipy.signal import welch #for frequency band\n",
    "from scipy.interpolate import interp1d #for interpolation of signal\n",
    "import neurokit2 as nk #for peak finding, and other measures\n",
    "from pyhrv.hrv import hrv #think neurokit might use this, needed otherwise issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_averages(parameter, health_state):\n",
    "    \"\"\"\n",
    "    calculates healthy and unhealthy means and std of parameter for easy comparisson\n",
    "    returns np.array containing: healthy mean, healthy std, unhealthy mean, unhealhty std\n",
    "    \"\"\"\n",
    "    encoded_health_state = [True if label == 'Unhealthy' else False for label in health_state]\n",
    "\n",
    "    unhealthy_param = parameter[np.array(encoded_health_state)]\n",
    "    healthy_param = parameter[~np.array(encoded_health_state)]\n",
    "    \n",
    "    unhealthy_param_av = np.mean(unhealthy_param)\n",
    "    unhealthy_param_std = np.std(unhealthy_param)\n",
    "    \n",
    "    healthy_param_av = np.mean(healthy_param)\n",
    "    healthy_param_std = np.std(healthy_param)\n",
    "    \n",
    "    return np.array([healthy_param_av, healthy_param_std, unhealthy_param_av, unhealthy_param_std])\n",
    "\n",
    "def print_averages(parameter, parameter_name, nan_indices):\n",
    "    \"\"\"\n",
    "    print healthy and unhealthy means of parameter in nice format\n",
    "    \"\"\"\n",
    "    health_state = allowed_patients.get_diagnoses()\n",
    "    \n",
    "    nan_indices = np.array(nan_indices)\n",
    "    health_state = np.array(health_state)\n",
    "    \n",
    "    channel_health_state = health_state[nan_indices]\n",
    "    \n",
    "    means = parameter_averages(parameter, channel_health_state)\n",
    "    \n",
    "    print(f\"Unhealthy {parameter_name}: mean:{means[2]}, std: {means[3]}\")\n",
    "    print(f\"Healthy {parameter_name}: mean:{means[0]}, std:{means[1]}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def outliers_indices_z_score(data, threshold=2):\n",
    "    \"\"\"\n",
    "    finds outliers from data, those outside the threshold region\n",
    "    return the indices of the data to be kept\n",
    "    \"\"\"\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    z_scores = [(x - mean) / std for x in data]\n",
    "    indices = np.argwhere([abs(z)<threshold for z in z_scores])\n",
    "    return indices\n",
    "\n",
    "def get_peaks(sig):\n",
    "    \"\"\"\n",
    "    returns the location of the peaks (calculated through neurokit) and their average amplitude\n",
    "    \"\"\"\n",
    "    #using neurokit to find the location of the r peaks\n",
    "    peak_dict, info = nk.ecg_peaks(sig, sampling_rate=1000)\n",
    "    peaks = info['ECG_R_Peaks']\n",
    "    \n",
    "    #calculating amplitude of R peak\n",
    "    peak_amp = sig[peaks] - np.median(sig) #median used as baseline\n",
    "    peak_amp_av = np.mean(peak_amp) \n",
    "    return peaks, peak_amp_av\n",
    "        \n",
    "\n",
    "def get_rri(sig, remove_outliers=False):\n",
    "    \"\"\"\n",
    "    calculates the rr intervals and peak amplitudes of a signal\n",
    "    has option to remove outliers from the rr interval array\n",
    "    \"\"\"\n",
    "    #gets rpeaks and amplitudes\n",
    "    peaks, amps = get_peaks(sig)\n",
    "    \n",
    "    # calculates rr intervals\n",
    "    rri = np.diff(peaks)\n",
    "    \n",
    "    #removal of outliers\n",
    "    if remove_outliers:\n",
    "        rri_outlier_indices = outliers_indices_z_score(rri).reshape(-1)\n",
    "        rr_intervals = rri[rri_outlier_indices]\n",
    "\n",
    "    else:\n",
    "        rr_intervals = rri\n",
    "    return rr_intervals, amps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the denoised signal arrays contains nan signals, we have to find the location of these so that they dont interfere with the parameter calculations. These are different for each channel as each channel has different signals that have been allowed to stay based on their signal quality,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-calculation of relevant health array for every channel\n",
    "nan_indices = []\n",
    "health_state = allowed_patients.get_diagnoses()\n",
    "for j in range(0, no_channels):\n",
    "    signal_nan_indices = []\n",
    "    for i, signal in enumerate(denoised_signals[:, j]):\n",
    "        if np.isnan(signal).all():\n",
    "            signal_nan_indices.append(False)\n",
    "        else:\n",
    "            signal_nan_indices.append(True)\n",
    "    nan_indices.append(signal_nan_indices)\n",
    "\n",
    "health_state = np.array(health_state)\n",
    "nan_indices = np.array(nan_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Time Domain Features\n"
     ]
    }
   ],
   "source": [
    "print('Calculating Time Domain Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RR signal analysis\n",
    "- RR means: mean of the rr intervals\n",
    "- RR stds: standard deviation of the rr intervals\n",
    "- R amplitudes: amplitudes of the r peaks\n",
    "- RMSSD: the root mean square of successive differences of rr intervals\n",
    "- pNN50s: the number of successive rr intervals that differ by more than 50ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RR_analysis(signal, remove_outliers=False):\n",
    "    \"\"\"\n",
    "    calculates parameters based on the RR intervals of the signal\n",
    "    returns mean of RR intervals, std of RR intervals, RMSSD, pNN50, amplitude of r peaks\n",
    "    \"\"\"\n",
    "    peak_distances, amps = get_rri(signal, remove_outliers=remove_outliers)\n",
    "    \n",
    "    # stastitical analysis on rr intervals\n",
    "    mean_RR = np.mean(peak_distances)\n",
    "    std_RR = np.std(peak_distances)\n",
    "\n",
    "    #RMSSD\n",
    "    # computes differences between successive RR intervals for RMSSD\n",
    "    diff_RR_intervals = np.diff(peak_distances)\n",
    "    \n",
    "    RMSSD_RR = np.sqrt(np.mean(diff_RR_intervals**2))\n",
    "    \n",
    "    #pNN50\n",
    "    # the number of successive RR intervals that differ by more than 50 ms \n",
    "    NN50 = np.sum(np.abs(diff_RR_intervals) > 50)\n",
    "    # divide by total number of RR intervals\n",
    "    pNN50 = (NN50 / len(peak_distances)) * 100\n",
    "    \n",
    "    \n",
    "    return mean_RR, std_RR, RMSSD_RR, pNN50, amps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating parameters for each signal in each channel\n",
    "rr_means_list = []\n",
    "rr_stds_list = []\n",
    "rr_RMSSD_list = []\n",
    "rr_pNN50s_list = []\n",
    "rr_amps_list = []\n",
    "\n",
    "for j in range(0, no_channels):\n",
    "    rr_means = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    rr_stds = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    rr_RMSSD = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    rr_pNN50s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    rr_amps = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "\n",
    "    i = 0\n",
    "    for signal in denoised_signals[:, j]:\n",
    "        if not np.isnan(signal).all():\n",
    "            rr_mean, rr_std, rr_rmssd, rr_pNN50, rr_amp = RR_analysis(signal, remove_outliers=True)\n",
    "\n",
    "            rr_means[i] = rr_mean\n",
    "            rr_stds[i] = rr_std\n",
    "            rr_RMSSD[i] = rr_rmssd\n",
    "            rr_pNN50s[i] = rr_pNN50\n",
    "            rr_amps[i] = rr_amp\n",
    "\n",
    "\n",
    "            i+=1\n",
    "    \n",
    "    rr_means_list.append(rr_means)\n",
    "    rr_stds_list.append(rr_stds)\n",
    "    rr_RMSSD_list.append(rr_RMSSD)\n",
    "    rr_pNN50s_list.append(rr_pNN50s)\n",
    "    rr_amps_list.append(rr_amps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QRS Complex\n",
    "QRS complex represents the duration of the main contraction of the heart. Here I have calculated both the mean and standard deviation of it. This has been done using neurokit2's ```ecg_delineate``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QRS_complex(signal):\n",
    "    #required nk cleaned signal\n",
    "    cleaned_signal = nk.ecg_clean(signal, sampling_rate=1000)\n",
    "    \n",
    "    #calculates location of all the various peaks in the signal\n",
    "    wave_dict, signals_df = nk.ecg_delineate(signal)\n",
    "        \n",
    "    q_peaks = signals_df['ECG_Q_Peaks']\n",
    "    s_peaks = signals_df['ECG_S_Peaks']\n",
    "        \n",
    "    #calculating duration of QRS complex\n",
    "    qrs = np.array(s_peaks) - np.array(q_peaks)\n",
    "\n",
    "    #calculating mean and std\n",
    "    qrs_mean = np.nanmean(qrs)\n",
    "    qrs_std = np.nanstd(qrs)\n",
    "    \n",
    "    return qrs_mean, qrs_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m signal \u001b[38;5;129;01min\u001b[39;00m denoised_signals[:, j]:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(signal)\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m---> 11\u001b[0m         QRS_mean, QRS_std \u001b[38;5;241m=\u001b[39m \u001b[43mQRS_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m         QRS_means[i] \u001b[38;5;241m=\u001b[39m QRS_mean\n\u001b[0;32m     14\u001b[0m         QRS_stds[i] \u001b[38;5;241m=\u001b[39m QRS_std\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mQRS_complex\u001b[1;34m(signal)\u001b[0m\n\u001b[0;32m      3\u001b[0m cleaned_signal \u001b[38;5;241m=\u001b[39m nk\u001b[38;5;241m.\u001b[39mecg_clean(signal, sampling_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#calculates location of all the various peaks in the signal\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m wave_dict, signals_df \u001b[38;5;241m=\u001b[39m \u001b[43mnk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mecg_delineate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m q_peaks \u001b[38;5;241m=\u001b[39m signals_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mECG_Q_Peaks\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m s_peaks \u001b[38;5;241m=\u001b[39m signals_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mECG_S_Peaks\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\court\\Documents\\anaconda\\envs\\ECG-project\\lib\\site-packages\\neurokit2\\ecg\\ecg_delineate.py:164\u001b[0m, in \u001b[0;36mecg_delineate\u001b[1;34m(ecg_cleaned, rpeaks, sampling_rate, method, show, show_type, check, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m     waves \u001b[38;5;241m=\u001b[39m _ecg_delineator_cwt(\n\u001b[0;32m    161\u001b[0m         ecg_cleaned, rpeaks\u001b[38;5;241m=\u001b[39mrpeaks, sampling_rate\u001b[38;5;241m=\u001b[39msampling_rate\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdwt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscrete wavelet transform\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 164\u001b[0m     waves \u001b[38;5;241m=\u001b[39m \u001b[43m_dwt_ecg_delineator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mecg_cleaned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrpeaks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeuroKit error: ecg_delineate(): \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeak\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcwt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdwt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\court\\Documents\\anaconda\\envs\\ECG-project\\lib\\site-packages\\neurokit2\\ecg\\ecg_delineate.py:270\u001b[0m, in \u001b[0;36m_dwt_ecg_delineator\u001b[1;34m(ecg, rpeaks, sampling_rate, analysis_sampling_rate)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# dwt to delineate tp waves, onsets, offsets and qrs ontsets and offsets\u001b[39;00m\n\u001b[0;32m    267\u001b[0m ecg \u001b[38;5;241m=\u001b[39m signal_resample(\n\u001b[0;32m    268\u001b[0m     ecg, sampling_rate\u001b[38;5;241m=\u001b[39msampling_rate, desired_sampling_rate\u001b[38;5;241m=\u001b[39manalysis_sampling_rate\n\u001b[0;32m    269\u001b[0m )\n\u001b[1;32m--> 270\u001b[0m dwtmatr \u001b[38;5;241m=\u001b[39m \u001b[43m_dwt_compute_multiscales\u001b[49m\u001b[43m(\u001b[49m\u001b[43mecg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# # only for debugging\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;66;03m# for idx in [0, 1, 2, 3]:\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m#     plt.plot(dwtmatr[idx + 3], label=f'W[{idx}]')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m# plt.grid(True)\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[0;32m    279\u001b[0m rpeaks_resampled \u001b[38;5;241m=\u001b[39m _dwt_resample_points(\n\u001b[0;32m    280\u001b[0m     rpeaks, sampling_rate, analysis_sampling_rate\n\u001b[0;32m    281\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\court\\Documents\\anaconda\\envs\\ECG-project\\lib\\site-packages\\neurokit2\\ecg\\ecg_delineate.py:675\u001b[0m, in \u001b[0;36m_dwt_compute_multiscales\u001b[1;34m(ecg, max_degree)\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m signal_f\n\u001b[0;32m    674\u001b[0m dwtmatr \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 675\u001b[0m intermediate_ret \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mecg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m deg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_degree):\n\u001b[0;32m    677\u001b[0m     S_deg \u001b[38;5;241m=\u001b[39m _apply_G_filter(intermediate_ret, power\u001b[38;5;241m=\u001b[39mdeg)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # #takes a long time so ignored for now\n",
    "# QRS_means_list = []\n",
    "# QRS_stds_list = []\n",
    "\n",
    "# for j in range(0, no_channels):\n",
    "#     QRS_means = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "#     QRS_stds = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "#     i = 0\n",
    "#     for signal in denoised_signals[:, j]:\n",
    "#         if not np.isnan(signal).all():\n",
    "#             QRS_mean, QRS_std = QRS_complex(signal)\n",
    "\n",
    "#             QRS_means[i] = QRS_mean\n",
    "#             QRS_stds[i] = QRS_std\n",
    "\n",
    "#             i+=1\n",
    "    \n",
    "#     QRS_means_list.append(QRS_means)\n",
    "#     QRS_stds_list.append(QRS_stds)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physiological measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moments(signal):\n",
    "    \"\"\"\n",
    "    returns the moments of the signal up to the fourth order\n",
    "    \"\"\"\n",
    "    mean = np.mean(signal)\n",
    "    std = np.std(signal)\n",
    "    skew_ecg = skew(signal)\n",
    "    kurtosis_ecg = kurtosis(signal)\n",
    "    return mean, std, skew_ecg, kurtosis_ecg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_list = []\n",
    "stds_list = []\n",
    "skews_list = []\n",
    "kurtosiss_list = []\n",
    "\n",
    "\n",
    "for j in range(0, no_channels):\n",
    "    means = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    stds = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    skews = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    kurtosiss = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    \n",
    "    i=0\n",
    "    for signal in denoised_signals[:, j]:\n",
    "        if not np.isnan(signal).all():\n",
    "\n",
    "            mean, std, ecg_skew, ecg_kurtosis = get_moments(signal)\n",
    "\n",
    "            means[i] = mean\n",
    "            stds[i] = std\n",
    "            skews[i] = ecg_skew\n",
    "            kurtosiss[i] = ecg_kurtosis\n",
    "            \n",
    "            i+=1\n",
    "\n",
    "    means_list.append(means)\n",
    "    stds_list.append(stds)\n",
    "    skews_list.append(skews)\n",
    "    kurtosiss_list.append(kurtosiss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Frequency Domain Features\n"
     ]
    }
   ],
   "source": [
    "print('Calculating Frequency Domain Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_bands(signal):\n",
    "    \"\"\"\n",
    "    performs welch transform to calculate the power within high and low frequency bands\n",
    "    as well as the total power within the frequency spectrum\n",
    "    \"\"\"\n",
    "    fs = 1000 #Hz, sampling frequency\n",
    "    fs_interpolate = 3 #Hz, arbitrary value, 3 is typically used\n",
    "    \n",
    "    # define frequency bands, based on typical values used in HRV\n",
    "    lf_band = (0.04, 0.15)\n",
    "    hf_band = (0.15, 0.40)\n",
    "    \n",
    "    #gets rpeaks and amplitudes\n",
    "    peaks, amps = get_peaks(signal)\n",
    "    \n",
    "    # calculates rr intervals in seconds\n",
    "    rr_intervals = np.diff(peaks)/fs\n",
    "    \n",
    "    # time points of the RR intervals\n",
    "    rr_times = np.cumsum(rr_intervals)\n",
    "    rr_times = np.insert(rr_times, 0, 0)  # add time zero at the beginning\n",
    "\n",
    "    # interpolation for frequency transform\n",
    "    interpolated_time = np.arange(0, rr_times[-2], 1/fs_interpolate)\n",
    "    interpolated_rr = interp1d(rr_times[:-1], rr_intervals, kind='cubic')(interpolated_time)\n",
    "\n",
    "    #welch spectrum\n",
    "    f, psd = welch(interpolated_rr, fs=fs_interpolate, nperseg=128)\n",
    "    \n",
    "    # integrate the power spectral density over the frequency bands\n",
    "    lf_power = np.trapz(psd[(f >= lf_band[0]) & (f <= lf_band[1])], f[(f >= lf_band[0]) & (f <= lf_band[1])])\n",
    "    hf_power = np.trapz(psd[(f >= hf_band[0]) & (f <= hf_band[1])], f[(f >= hf_band[0]) & (f <= hf_band[1])])\n",
    "    \n",
    "    #total power, integrated over whole range\n",
    "    total_power = np.trapz(psd, f)\n",
    "    \n",
    "    return lf_power, hf_power, lf_power/hf_power, total_power\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_list = []\n",
    "hfs_list = []\n",
    "ratios_list = []\n",
    "tps_list = []\n",
    "\n",
    "for j in range(0, no_channels):\n",
    "    lfs = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    hfs = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    ratios = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    tps = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    \n",
    "    i=0\n",
    "    for signal in denoised_signals[:, j]:\n",
    "        if not np.isnan(signal).all():\n",
    "            lf, hf, ratio, tp = power_bands(signal)\n",
    "            lfs[i] = lf\n",
    "            hfs[i] = hf\n",
    "            ratios[i] = ratio\n",
    "            tps[i] = tp\n",
    "            \n",
    "            i+=1\n",
    "    lfs_list.append(lfs)\n",
    "    hfs_list.append(hfs)\n",
    "    ratios_list.append(ratios)\n",
    "    tps_list.append(tps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Non-Linear Domain Features\n"
     ]
    }
   ],
   "source": [
    "print('Calculating Non-Linear Domain Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poincare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_poincare_sd(sig, remove_outliers=False, use_nk=False):\n",
    "    #get rr intervals\n",
    "    rr_intervals = get_rri(sig)[0]\n",
    "    \n",
    "    if remove_outliers:\n",
    "        rr_intervals = rr_intervals[outliers_indices_z_score(rr_intervals)]\n",
    "        \n",
    "    if use_nk:\n",
    "        peaks, info = nk.ecg_peaks(sig, sampling_rate=1000)\n",
    "        nl_indices = nk.hrv_nonlinear(peaks, sampling_rate=1000, show=False)\n",
    "        return nl_indices['HRV_SD1'], nl_indices['HRV_SD2'], nl_indices['HRV_SD1SD2']\n",
    "        \n",
    "    #separating into subsequent coordinates for Poincaré plot\n",
    "    rr_n = rr_intervals[:-1]\n",
    "    rr_n1 = rr_intervals[1:]\n",
    "    \n",
    "    #calculating SD1, perpendicular to y=x\n",
    "    diff_rr = np.array(rr_n) - np.array(rr_n1)\n",
    "    sd1 = np.sqrt(np.var(diff_rr/np.sqrt(2)))\n",
    "    \n",
    "    # calculating SD2, along y=x\n",
    "    sum_rr = rr_n + rr_n1\n",
    "    sd2 = np.sqrt(np.var(sum_rr/np.sqrt(2)))\n",
    "    \n",
    "    # calculating ratio\n",
    "    sd_ratio = sd2/sd1\n",
    "   \n",
    "    if remove_outliers:\n",
    "        #counting intervals outside SD1 and SD2\n",
    "        count_outside_sd1 = np.sum(np.abs(diff_rr / np.sqrt(2)) > sd1)\n",
    "        count_outside_sd2 = np.sum(np.abs(sum_rr / np.sqrt(2)) > sd2)\n",
    "\n",
    "        out = count_outside_sd1 + count_outside_sd2\n",
    "        return sd1, sd2, sd_ratio, count_outside_sd1, count_outside_sd2, out\n",
    "    \n",
    "    else:\n",
    "        return sd1, sd2, sd_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sd1s_outliers_removed_list = []\n",
    "# sd2s_outliers_removed_list = []\n",
    "# sd_ratios_outliers_removed_list = []\n",
    "# out_sd1s_list = []\n",
    "# out_sd2s_list = []\n",
    "# n_out_list = []\n",
    "# for j in range(0, no_channels):\n",
    "#     sd1s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "#     sd2s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "#     sd_ratios = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "#     out_sd1s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "#     out_sd2s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "#     n_out = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "\n",
    "#     i = 0\n",
    "#     for signal in denoised_signals[:, j]:\n",
    "#         if not np.isnan(signal).all():\n",
    "#             sd1, sd2, sd_ratio, out_sd1, out_sd2, out = calculate_poincare_sd(signal, remove_outliers=True)\n",
    "#             sd1s[i] = sd1\n",
    "#             sd2s[i] = sd2\n",
    "#             sd_ratios[i] = sd_ratio\n",
    "#             out_sd1s[i] = out_sd1\n",
    "#             out_sd2s[i] = out_sd2\n",
    "#             n_out[i] = out\n",
    "\n",
    "#             i+=1\n",
    "        \n",
    "#     sd1s_outliers_removed_list.append(sd1s)\n",
    "#     sd2s_outliers_removed_list.append(sd2s)\n",
    "#     sd_ratios_outliers_removed_list.append(sd_ratios)\n",
    "#     out_sd1s_list.append(out_sd1s)\n",
    "#     out_sd2s_list.append(out_sd2s)\n",
    "#     n_out_list.append(n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd1s_list = []\n",
    "sd2s_list = []\n",
    "sd_ratios_list = []\n",
    "\n",
    "for j in range(0, no_channels):\n",
    "    sd1s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    sd2s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    sd_ratios = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "\n",
    "    i = 0\n",
    "    for signal in denoised_signals[:, j]:\n",
    "        if not np.isnan(signal).all():\n",
    "            sd1, sd2, sd_ratio = calculate_poincare_sd(signal, remove_outliers=False)\n",
    "            sd1s[i] = sd1\n",
    "            sd2s[i] = sd2\n",
    "            sd_ratios[i] = sd_ratio\n",
    "\n",
    "            i+=1\n",
    "        \n",
    "    sd1s_list.append(sd1s)\n",
    "    sd2s_list.append(sd2s)\n",
    "    sd_ratios_list.append(sd_ratios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #takes a while\n",
    "# nk_sd1s_list = []\n",
    "# nk_sd2s_list = []\n",
    "# nk_sd_ratios_list = []\n",
    "\n",
    "# for j in range(0, no_channels):\n",
    "#     nk_sd1s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "#     nk_sd2s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "#     nk_sd_ratios = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "\n",
    "#     i = 0\n",
    "#     for signal in denoised_signals[:, j]:\n",
    "#         if not np.isnan(signal).all():\n",
    "#             sd1, sd2, sd_ratio = calculate_poincare_sd(signal, remove_outliers=False, use_nk=True)\n",
    "#             nk_sd1s[i] = sd1\n",
    "#             nk_sd2s[i] = sd2\n",
    "#             nk_sd_ratios[i] = sd_ratio\n",
    "\n",
    "#             i+=1\n",
    "        \n",
    "#     nk_sd1s_list.append(nk_sd1s)\n",
    "#     nk_sd2s_list.append(nk_sd2s)\n",
    "#     nk_sd_ratios_list.append(nk_sd_ratios)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shannon_entropy(signal, num_bins=500, use_nk=False):\n",
    "    rr_intervals = get_rri(signal)[0]\n",
    "    \n",
    "    if use_nk:\n",
    "        peaks, info = nk.ecg_peaks(signal, sampling_rate=1000)\n",
    "        nl_indices = nk.hrv_nonlinear(peaks, sampling_rate=1000, show=False)\n",
    "        return nl_indices['HRV_ShanEn']\n",
    "    \n",
    "    # discretize RR intervals into bins and calculate probabilities\n",
    "    hist, bin_edges = np.histogram(rr_intervals, bins=num_bins, density=True)\n",
    "    \n",
    "    probabilities = hist / np.sum(hist)\n",
    "    \n",
    "    # Calculate Shannon Entropy using equation\n",
    "    shannon_entropy = -np.sum(probabilities * np.log2(probabilities + 1e-12))  # adding a small value to avoid log(0)\n",
    "    \n",
    "    return shannon_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have nk option here aswell\n",
    "shannon_ens_list = []\n",
    "\n",
    "for j in range(0, no_channels):\n",
    "    shannon_ens = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "\n",
    "    i=0\n",
    "    for signal in denoised_signals[:, j]:\n",
    "        if not np.isnan(signal).all():\n",
    "            shannon_ens[i] = calculate_shannon_entropy(signal)\n",
    "\n",
    "            i+=1\n",
    "        \n",
    "    shannon_ens_list.append(shannon_ens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_entropy(signal, m=2, r=0.2):\n",
    "    rr_intervals = get_rri(signal)[0]\n",
    "    N = len(rr_intervals)\n",
    "    r *= np.std(rr_intervals)  # tolerance r is usually set as a fraction of the standard deviation\n",
    "    \n",
    "    def _phi(m):\n",
    "        X = np.array([rr_intervals[i:i + m] for i in range(N - m + 1)])\n",
    "        C = np.sum(np.max(np.abs(X[:, None] - X[None, :]), axis=2) <= r, axis=0) - 1\n",
    "        return np.sum(C) / (N - m + 1)\n",
    "    \n",
    "    return -np.log((_phi(m + 1)) / (_phi(m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_ens_list = []\n",
    "# for j in range(0, no_channels):\n",
    "#     samp_ens = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "#     i=0\n",
    "#     for signal in denoised_signals[:, j]:\n",
    "#         if not np.isnan(signal).all():\n",
    "#             samp_en = calculate_sample_entropy(signal)\n",
    "#             if samp_en == np.inf:\n",
    "#                 samp_en = np.nan\n",
    "#             samp_ens[i] = samp_en\n",
    "            \n",
    "#             i+=1\n",
    "#     samp_ens_list.append(samp_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higuchi FD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_higuchi_fd(time_series, k_max = 50):\n",
    "    \"\"\"\n",
    "    Calculate the fractal dimension of a time series using Higuchi's algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    - time_series: The input time series as a 1D numpy array.\n",
    "    - k_max: The maximum value of k (the parameter that controls segment length), would want a value of k_max roughly 10% of signal length but this would take forever\n",
    "    \n",
    "    Returns:\n",
    "    - The estimated fractal dimension.\n",
    "    \"\"\"\n",
    "    N = len(time_series)\n",
    "    L = np.zeros(k_max)\n",
    "    x = np.arange(N)\n",
    "\n",
    "    for k in range(1, k_max + 1):\n",
    "        Lk = np.zeros(k)\n",
    "\n",
    "        for m in range(0, k):\n",
    "            Lmk = 0\n",
    "            for i in range(1, int((N - m) / k)):\n",
    "                Lmk += abs(time_series[m + i * k] - time_series[m + (i - 1) * k])\n",
    "            Lmk = (Lmk * (N - 1) / (int((N - m) / k) * k)) / k\n",
    "            Lk[m] = Lmk\n",
    "\n",
    "        L[k - 1] = np.mean(Lk)\n",
    "\n",
    "    # Perform linear fit in log-log scale\n",
    "    ln_k = np.log(range(1, k_max + 1))\n",
    "    ln_L = np.log(L)\n",
    "    coeffs = np.polyfit(ln_k, ln_L, 1)\n",
    "\n",
    "    # The slope of the line is the fractal dimension\n",
    "    fractal_dimension = -coeffs[0]\n",
    "    \n",
    "    return fractal_dimension, coeffs, ln_k, ln_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [42:43<00:00, 427.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# #this takes roughly 40 mins so include at your discresion, even with very inoptimal value of kmax\n",
    "# from tqdm import tqdm\n",
    "# higuchi_fds_list = []\n",
    "\n",
    "# for j in tqdm(range(0, no_channels)):\n",
    "#     higuchi_fds = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "#     i=0\n",
    "#     for signal in denoised_signals[:, j]:\n",
    "#         if not np.isnan(signal).all():\n",
    "#             higuchi_fds[i] = calculate_higuchi_fd(signal)[0]\n",
    "\n",
    "#             i+=1\n",
    "        \n",
    "#     higuchi_fds_list.append(higuchi_fds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fractal Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating autocorrelation and time delay\n",
    "def autocorrelation(signal):\n",
    "    n = len(signal)\n",
    "    mean = np.mean(signal)\n",
    "    var = np.var(signal)\n",
    "    signal = signal - mean\n",
    "    autocorr = correlate(signal, signal, mode='full')[n-1:] / (var * n) #scipy correlate alot faster\n",
    "    return autocorr\n",
    "\n",
    "def find_time_delay(signal):\n",
    "    autocorr = autocorrelation(signal)\n",
    "    # Find the point where autocorrelation drops to 1/e of its initial value\n",
    "    threshold = 1 / np.exp(1)\n",
    "    tau = np.argmax(autocorr <= threshold) #measured in units of sampling rate (1000 hz)\n",
    "    return tau\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_list = []\n",
    "for j in range(0, no_channels):\n",
    "    tau = np.zeros(shape = len(health_state[nan_indices[j]]), dtype=int)\n",
    "    i = 0\n",
    "    for signal in denoised_signals[:, j]:\n",
    "        if not np.isnan(signal).all():\n",
    "            tau[i] = find_time_delay(signal)\n",
    "            \n",
    "            i+=1\n",
    "    tau_list.append(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding time series\n",
    "def embed_time_series(signal, tau, m):\n",
    "    n = len(signal)\n",
    "    if n < (m - 1) * tau:\n",
    "        raise ValueError(\"Time series is too short for the chosen tau and embedding dimension.\")\n",
    "    embedded = np.array([signal[i : i + (m - 1) * tau + 1 : tau] for i in range(n - (m - 1) * tau)])\n",
    "    return embedded\n",
    "\n",
    "# embedding dimension M\n",
    "M = 4\n",
    "\n",
    "    \n",
    "#creates list of embedded arrays i.e. no_patients * len(embedded) * M\n",
    "\n",
    "embedded_signals_list = []\n",
    "for j in range(0, no_channels):\n",
    "    embedded_signals = []\n",
    "    i = 0\n",
    "    for signal in denoised_signals[:, j]:\n",
    "        if not np.isnan(signal).all():\n",
    "            embedded_signals.append(embed_time_series(signal, tau_list[j][i], M))\n",
    "            \n",
    "            i+=1\n",
    "    embedded_signals_list.append(embedded_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating fractal dimension through box counting\n",
    "def box_counting(data, box_sizes):\n",
    "    counts = []\n",
    "    for size in box_sizes:\n",
    "        count = 0\n",
    "        # creates grid\n",
    "        grid = np.ceil(data / size).astype(int)\n",
    "        #counts unique boxes\n",
    "        unique_boxes = np.unique(grid, axis=0)\n",
    "        count = len(unique_boxes)\n",
    "        counts.append(count)\n",
    "    return counts\n",
    "\n",
    "def fractal_dimension(data, box_sizes):\n",
    "    counts = box_counting(data, box_sizes)\n",
    "    log_box_sizes = np.log(box_sizes)\n",
    "    log_counts = np.log(counts)\n",
    "    \n",
    "    # Perform linear regression to find the slope of the log-log plot\n",
    "    coeffs = np.polyfit(log_box_sizes, log_counts, 1)\n",
    "    return -coeffs[0]  # The fractal dimension is the negative slope\n",
    "\n",
    "#check this whole code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fractal_dim(signal):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    embedded_signal_std = scaler.fit_transform(signal)\n",
    "\n",
    "    # performs SVD and obtains principal components\n",
    "    U, S, VT = svd(embedded_signal_std, full_matrices=False)\n",
    "    PCs = U @ np.diag(S)\n",
    "    \n",
    "    # define box sizes\n",
    "    box_sizes = np.logspace(-1, 1, num=10)  # research/try this out\n",
    "    \n",
    "    # calculate the fractal dimension\n",
    "    fd = fractal_dimension(PCs[:, :3], box_sizes)  # uses the first three principal components, maybe use more??\n",
    "    return fd    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #takes a long time so will leave it out for now \n",
    "# fd_list = []\n",
    "# for j in range(0, no_channels):\n",
    "#     fd = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "#     i=0\n",
    "#     for signal in embedded_signals_list[j]:\n",
    "#         fd[i] = find_time_delay(signal)\n",
    "            \n",
    "#         i+=1\n",
    "#     fd_list.append(fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariates\n",
    "- have access to age and gender, age is the only one that makes sense to investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages=np.zeros(no_patients)\n",
    "for i in range(0, no_patients):\n",
    "    ages[i] = allowed_patients.get_patients(i).get_age()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages_list = []\n",
    "for j in range(0, no_channels):\n",
    "    ages_list.append(ages[nan_indices[j]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Parameter Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create dictionary with parameters\n",
    "params = {}\n",
    "\n",
    "#time components\n",
    "params['RR mean'] = rr_means_list\n",
    "params['RR std'] = rr_stds_list\n",
    "params['RR amps'] = rr_amps_list\n",
    "params['RMSSD'] = rr_RMSSD_list\n",
    "params['pNN50'] = rr_pNN50s_list\n",
    "\n",
    "#params['QRS_mean'] = QRS_means_list #contains nan so need to remove those so can use the filtering methods\n",
    "#params['QRS_std'] = QRS_stds_list\n",
    "\n",
    "params['mean'] = means_list\n",
    "params['std'] = stds_list\n",
    "params['skews'] = skews_list\n",
    "params['kurtosis'] = kurtosiss_list\n",
    "\n",
    "#frequency components\n",
    "params['hf'] = hfs_list\n",
    "params['lf'] = lfs_list\n",
    "params['power ratio'] = ratios_list\n",
    "params['total power'] = tps_list\n",
    "\n",
    "#nonlinear components\n",
    "params['shannon en'] = shannon_ens_list\n",
    "#params['sample_en'] = samp_ens_list need to do imputing if want to include it, due to some nan values\n",
    "params['sd ratio'] = sd_ratios_list\n",
    "params['sd1'] = sd1s_list\n",
    "params['sd2'] = sd2s_list\n",
    "#params['nk_sd_ratio'] = nk_sd_ratios_list\n",
    "#params['sd_ratio_outliers_removed'] = sd_ratios_outliers_removed_list\n",
    "#params['multi fd'] = fd_list\n",
    "#params['higuchi fd'] = higuchi_fds_list\n",
    "\n",
    "#covariates\n",
    "params['age'] = ages_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
