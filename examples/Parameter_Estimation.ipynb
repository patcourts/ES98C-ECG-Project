{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes access to detrended, denoised signals and calculates various parameters from these signals. The parameters estimated within this notebook are split into three broad categories: time domain, frequency-domain and non-linear domain. They have been chosen due to their successes in literature relating to ECG classification as well as their use as Heart rate variability (HRV) metrics:\n",
    "\n",
    "Time domain:\n",
    " - mean RR intervals\n",
    " - std RR intervals \n",
    " - mean amplitude\n",
    " - RMSSD of differences between successive RR intervals \n",
    " - pNN50 (%) NN50 divided by total number of RR (NN50 is the number of sucessive RR intervals that differ by more than 50ms) \n",
    " - mean QRS duration\n",
    " - std QRS duration\n",
    " - moments of the signal (physiological measures)\n",
    " \n",
    " \n",
    "Frequency Domain:\n",
    "- absolute power of LF (0.04 - 0.15 Hz) band\n",
    "- absolute power of HF (0.15 to 0.4Hz) band\n",
    "- LF/HF ratio\n",
    "- Total power (0-0.4 Hz)\n",
    "\n",
    "Non-linear:\n",
    "- Fractal dimension of dynamic attractor of signal\n",
    "- std of poincare\n",
    "- Shannon entropy\n",
    "- sample entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 549/549 [01:23<00:00,  6.57it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 221/221 [00:00<00:00, 342.64it/s]\n"
     ]
    }
   ],
   "source": [
    "%run \"Data_PreProcessing.ipynb\" #allowing access to the filtered database with preprocessed signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import correlate #for autocorrelation for signal embedding\n",
    "from scipy.stats import skew, kurtosis #for signal measures\n",
    "from scipy.signal import welch #for frequency band\n",
    "from scipy.interpolate import interp1d #for interpolation of signal\n",
    "import neurokit2 as nk #for peak finding, and other measures\n",
    "from pyhrv.hrv import hrv #think neurokit might use this, needed otherwise issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_averages(parameter, health_state):\n",
    "    \"\"\"\n",
    "    calculates healthy and unhealthy means and std of parameter for easy comparisson\n",
    "    returns np.array containing: healthy mean, healthy std, unhealthy mean, unhealhty std\n",
    "    \"\"\"\n",
    "    encoded_health_state = [True if label == 'Unhealthy' else False for label in health_state]\n",
    "\n",
    "    unhealthy_param = parameter[np.array(encoded_health_state)]\n",
    "    healthy_param = parameter[~np.array(encoded_health_state)]\n",
    "    \n",
    "    unhealthy_param_av = np.mean(unhealthy_param)\n",
    "    unhealthy_param_std = np.std(unhealthy_param)\n",
    "    \n",
    "    healthy_param_av = np.mean(healthy_param)\n",
    "    healthy_param_std = np.std(healthy_param)\n",
    "    \n",
    "    return np.array([healthy_param_av, healthy_param_std, unhealthy_param_av, unhealthy_param_std])\n",
    "\n",
    "def print_averages(parameter, parameter_name, nan_indices):\n",
    "    \"\"\"\n",
    "    print healthy and unhealthy means of parameter in nice format\n",
    "    \"\"\"\n",
    "    health_state = allowed_patients.get_diagnoses()\n",
    "    \n",
    "    nan_indices = np.array(nan_indices)\n",
    "    health_state = np.array(health_state)\n",
    "    \n",
    "    channel_health_state = health_state[nan_indices]\n",
    "    \n",
    "    means = parameter_averages(parameter, channel_health_state)\n",
    "    \n",
    "    print(f\"Unhealthy {parameter_name}: mean:{means[2]}, std: {means[3]}\")\n",
    "    print(f\"Healthy {parameter_name}: mean:{means[0]}, std:{means[1]}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def outliers_indices_z_score(data, threshold=2):\n",
    "    \"\"\"\n",
    "    finds outliers from data, those outside the threshold region\n",
    "    return the indices of the data to be kept\n",
    "    \"\"\"\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    z_scores = [(x - mean) / std for x in data]\n",
    "    indices = np.argwhere([abs(z)<threshold for z in z_scores])\n",
    "    return indices\n",
    "\n",
    "def get_peaks(sig):\n",
    "    \"\"\"\n",
    "    returns the location of the peaks (calculated through neurokit) and their average amplitude\n",
    "    \"\"\"\n",
    "    #using neurokit to find the location of the r peaks\n",
    "    peak_dict, info = nk.ecg_peaks(sig, sampling_rate=1000)\n",
    "    peaks = info['ECG_R_Peaks']\n",
    "    \n",
    "    #calculating amplitude of R peak\n",
    "    peak_amp = sig[peaks] - np.median(sig) #median used as baseline\n",
    "    peak_amp_av = np.mean(peak_amp) \n",
    "    return peaks, peak_amp_av\n",
    "        \n",
    "\n",
    "def get_rri(sig, remove_outliers=False):\n",
    "    \"\"\"\n",
    "    calculates the rr intervals and peak amplitudes of a signal\n",
    "    has option to remove outliers from the rr interval array\n",
    "    \"\"\"\n",
    "    #gets rpeaks and amplitudes\n",
    "    peaks, amps = get_peaks(sig)\n",
    "    \n",
    "    # calculates rr intervals\n",
    "    rri = np.diff(peaks)\n",
    "    \n",
    "    #removal of outliers\n",
    "    if remove_outliers:\n",
    "        rri_outlier_indices = outliers_indices_z_score(rri).reshape(-1)\n",
    "        rr_intervals = rri[rri_outlier_indices]\n",
    "\n",
    "    else:\n",
    "        rr_intervals = rri\n",
    "    return rr_intervals, amps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the denoised signal arrays contains nan signals, we have to find the location of these so that they dont interfere with the parameter calculations. These are different for each channel as each channel has different signals that have been allowed to stay based on their signal quality,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-calculation of relevant health array for every channel\n",
    "nan_indices = []\n",
    "health_state = allowed_patients.get_diagnoses()\n",
    "for j in range(0, no_channels):\n",
    "    signal_nan_indices = []\n",
    "    for i, signal in enumerate(denoised_signals[:, j]):\n",
    "        if np.isnan(signal).all():\n",
    "            signal_nan_indices.append(False)\n",
    "        else:\n",
    "            signal_nan_indices.append(True)\n",
    "    nan_indices.append(signal_nan_indices)\n",
    "\n",
    "health_state = np.array(health_state)\n",
    "nan_indices = np.array(nan_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Time Domain Features\n"
     ]
    }
   ],
   "source": [
    "print('Calculating Time Domain Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RR signal analysis\n",
    "- RR means: mean of the rr intervals\n",
    "- RR stds: standard deviation of the rr intervals\n",
    "- R amplitudes: amplitudes of the r peaks\n",
    "- RMSSD: the root mean square of successive differences of rr intervals\n",
    "- pNN50s: the number of successive rr intervals that differ by more than 50ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RR_analysis(signal, remove_outliers=False):\n",
    "    \"\"\"\n",
    "    calculates parameters based on the RR intervals of the signal\n",
    "    returns mean of RR intervals, std of RR intervals, RMSSD, pNN50, amplitude of r peaks\n",
    "    \"\"\"\n",
    "    peak_distances, amps = get_rri(signal, remove_outliers=remove_outliers)\n",
    "    \n",
    "    # stastitical analysis on rr intervals\n",
    "    mean_RR = np.mean(peak_distances)\n",
    "    std_RR = np.std(peak_distances)\n",
    "\n",
    "    #RMSSD\n",
    "    # computes differences between successive RR intervals for RMSSD\n",
    "    diff_RR_intervals = np.diff(peak_distances)\n",
    "    \n",
    "    RMSSD_RR = np.sqrt(np.mean(diff_RR_intervals**2))\n",
    "    \n",
    "    #pNN50\n",
    "    # the number of successive RR intervals that differ by more than 50 ms \n",
    "    NN50 = np.sum(np.abs(diff_RR_intervals) > 50)\n",
    "    # divide by total number of RR intervals\n",
    "    pNN50 = (NN50 / len(peak_distances)) * 100\n",
    "    \n",
    "    \n",
    "    return mean_RR, std_RR, RMSSD_RR, pNN50, amps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating parameters for each signal in each channel\n",
    "rr_means_list = []\n",
    "rr_stds_list = []\n",
    "rr_RMSSD_list = []\n",
    "rr_pNN50s_list = []\n",
    "rr_amps_list = []\n",
    "\n",
    "for j in range(0, no_channels):\n",
    "    rr_means = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    rr_stds = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    rr_RMSSD = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    rr_pNN50s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    rr_amps = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "\n",
    "    i = 0\n",
    "    for signal in denoised_signals[:, j]:\n",
    "        if not np.isnan(signal).all():\n",
    "            rr_mean, rr_std, rr_rmssd, rr_pNN50, rr_amp = RR_analysis(signal, remove_outliers=True)\n",
    "\n",
    "            rr_means[i] = rr_mean\n",
    "            rr_stds[i] = rr_std\n",
    "            rr_RMSSD[i] = rr_rmssd\n",
    "            rr_pNN50s[i] = rr_pNN50\n",
    "            rr_amps[i] = rr_amp\n",
    "\n",
    "\n",
    "            i+=1\n",
    "    \n",
    "    rr_means_list.append(rr_means)\n",
    "    rr_stds_list.append(rr_stds)\n",
    "    rr_RMSSD_list.append(rr_RMSSD)\n",
    "    rr_pNN50s_list.append(rr_pNN50s)\n",
    "    rr_amps_list.append(rr_amps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QRS Complex\n",
    "QRS complex represents the duration of the main contraction of the heart. Here I have calculated both the mean and standard deviation of it. This has been done using neurokit2's ```ecg_delineate``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QRS_complex(signal):\n",
    "    #required nk cleaned signal\n",
    "    cleaned_signal = nk.ecg_clean(signal, sampling_rate=1000)\n",
    "    \n",
    "    #calculates location of all the various peaks in the signal\n",
    "    wave_dict, signals_df = nk.ecg_delineate(signal)\n",
    "        \n",
    "    q_peaks = signals_df['ECG_Q_Peaks']\n",
    "    s_peaks = signals_df['ECG_S_Peaks']\n",
    "        \n",
    "    #calculating duration of QRS complex\n",
    "    qrs = np.array(s_peaks) - np.array(q_peaks)\n",
    "\n",
    "    #calculating mean and std\n",
    "    qrs_mean = np.nanmean(qrs)\n",
    "    qrs_std = np.nanstd(qrs)\n",
    "    \n",
    "    return qrs_mean, qrs_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #takes a long time so ignored for now\n",
    "# QRS_means_list = []\n",
    "# QRS_stds_list = []\n",
    "\n",
    "# for j in range(0, no_channels):\n",
    "#     QRS_means = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "#     QRS_stds = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "#     i = 0\n",
    "#     for signal in denoised_signals[:, j]:\n",
    "#         if not np.isnan(signal).all():\n",
    "#             QRS_mean, QRS_std = QRS_complex(signal)\n",
    "\n",
    "#             QRS_means[i] = QRS_mean\n",
    "#             QRS_stds[i] = QRS_std\n",
    "\n",
    "#             i+=1\n",
    "    \n",
    "#     QRS_means_list.append(QRS_means)\n",
    "#     QRS_stds_list.append(QRS_stds)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physiological measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moments(signal):\n",
    "    \"\"\"\n",
    "    returns the moments of the signal up to the fourth order\n",
    "    \"\"\"\n",
    "    mean = np.mean(signal)\n",
    "    std = np.std(signal)\n",
    "    skew_ecg = skew(signal)\n",
    "    kurtosis_ecg = kurtosis(signal)\n",
    "    return mean, std, skew_ecg, kurtosis_ecg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_list = []\n",
    "stds_list = []\n",
    "skews_list = []\n",
    "kurtosiss_list = []\n",
    "\n",
    "\n",
    "for j in range(0, no_channels):\n",
    "    means = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    stds = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    skews = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    kurtosiss = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    \n",
    "    i=0\n",
    "    for signal in denoised_signals[:, j]:\n",
    "        if not np.isnan(signal).all():\n",
    "\n",
    "            mean, std, ecg_skew, ecg_kurtosis = get_moments(signal)\n",
    "\n",
    "            means[i] = mean\n",
    "            stds[i] = std\n",
    "            skews[i] = ecg_skew\n",
    "            kurtosiss[i] = ecg_kurtosis\n",
    "            \n",
    "            i+=1\n",
    "\n",
    "    means_list.append(means)\n",
    "    stds_list.append(stds)\n",
    "    skews_list.append(skews)\n",
    "    kurtosiss_list.append(kurtosiss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Frequency Domain Features\n"
     ]
    }
   ],
   "source": [
    "print('Calculating Frequency Domain Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_bands(signal):\n",
    "    \"\"\"\n",
    "    performs welch transform to calculate the power within high and low frequency bands\n",
    "    as well as the total power within the frequency spectrum\n",
    "    \"\"\"\n",
    "    fs = 1000 #Hz, sampling frequency\n",
    "    fs_interpolate = 3 #Hz, arbitrary value, 3 is typically used\n",
    "    \n",
    "    # define frequency bands, based on typical values used in HRV\n",
    "    lf_band = (0.04, 0.15)\n",
    "    hf_band = (0.15, 0.40)\n",
    "    \n",
    "    #gets rpeaks and amplitudes\n",
    "    peaks, amps = get_peaks(signal)\n",
    "    \n",
    "    # calculates rr intervals in seconds\n",
    "    rr_intervals = np.diff(peaks)/fs\n",
    "    \n",
    "    # time points of the RR intervals\n",
    "    rr_times = np.cumsum(rr_intervals)\n",
    "    rr_times = np.insert(rr_times, 0, 0)  # add time zero at the beginning\n",
    "\n",
    "    # interpolation for frequency transform\n",
    "    interpolated_time = np.arange(0, rr_times[-2], 1/fs_interpolate)\n",
    "    interpolated_rr = interp1d(rr_times[:-1], rr_intervals, kind='cubic')(interpolated_time)\n",
    "\n",
    "    #welch spectrum\n",
    "    f, psd = welch(interpolated_rr, fs=fs_interpolate, nperseg=128)\n",
    "    \n",
    "    # integrate the power spectral density over the frequency bands\n",
    "    lf_power = np.trapz(psd[(f >= lf_band[0]) & (f <= lf_band[1])], f[(f >= lf_band[0]) & (f <= lf_band[1])])\n",
    "    hf_power = np.trapz(psd[(f >= hf_band[0]) & (f <= hf_band[1])], f[(f >= hf_band[0]) & (f <= hf_band[1])])\n",
    "    \n",
    "    #total power, integrated over whole range\n",
    "    total_power = np.trapz(psd, f)\n",
    "    \n",
    "    return lf_power, hf_power, lf_power/hf_power, total_power\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs_list = []\n",
    "hfs_list = []\n",
    "ratios_list = []\n",
    "tps_list = []\n",
    "\n",
    "for j in range(0, no_channels):\n",
    "    lfs = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    hfs = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    ratios = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    tps = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    \n",
    "    i=0\n",
    "    for signal in denoised_signals[:, j]:\n",
    "        if not np.isnan(signal).all():\n",
    "            lf, hf, ratio, tp = power_bands(signal)\n",
    "            lfs[i] = lf\n",
    "            hfs[i] = hf\n",
    "            ratios[i] = ratio\n",
    "            tps[i] = tp\n",
    "            \n",
    "            i+=1\n",
    "    lfs_list.append(lfs)\n",
    "    hfs_list.append(hfs)\n",
    "    ratios_list.append(ratios)\n",
    "    tps_list.append(tps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Non-Linear Domain Features\n"
     ]
    }
   ],
   "source": [
    "print('Calculating Non-Linear Domain Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poincare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_poincare_sd(sig, remove_outliers=False, use_nk=False):\n",
    "    #get rr intervals\n",
    "    rr_intervals = get_rri(sig)[0]\n",
    "    \n",
    "    if remove_outliers:\n",
    "        rr_intervals = rr_intervals[outliers_indices_z_score(rr_intervals)]\n",
    "        \n",
    "    if use_nk:\n",
    "        peaks, info = nk.ecg_peaks(sig, sampling_rate=1000)\n",
    "        nl_indices = nk.hrv_nonlinear(peaks, sampling_rate=1000, show=False)\n",
    "        return nl_indices['HRV_SD1'], nl_indices['HRV_SD2'], nl_indices['HRV_SD1SD2']\n",
    "        \n",
    "    #separating into subsequent coordinates for Poincaré plot\n",
    "    rr_n = rr_intervals[:-1]\n",
    "    rr_n1 = rr_intervals[1:]\n",
    "    \n",
    "    #calculating SD1, perpendicular to y=x\n",
    "    diff_rr = np.array(rr_n) - np.array(rr_n1)\n",
    "    sd1 = np.sqrt(np.var(diff_rr/np.sqrt(2)))\n",
    "    \n",
    "    # calculating SD2, along y=x\n",
    "    sum_rr = rr_n + rr_n1\n",
    "    sd2 = np.sqrt(np.var(sum_rr/np.sqrt(2)))\n",
    "    \n",
    "    # calculating ratio\n",
    "    sd_ratio = sd2/sd1\n",
    "   \n",
    "    if remove_outliers:\n",
    "        #counting intervals outside SD1 and SD2\n",
    "        count_outside_sd1 = np.sum(np.abs(diff_rr / np.sqrt(2)) > sd1)\n",
    "        count_outside_sd2 = np.sum(np.abs(sum_rr / np.sqrt(2)) > sd2)\n",
    "\n",
    "        out = count_outside_sd1 + count_outside_sd2\n",
    "        return sd1, sd2, sd_ratio, count_outside_sd1, count_outside_sd2, out\n",
    "    \n",
    "    else:\n",
    "        return sd1, sd2, sd_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd1s_outliers_removed_list = []\n",
    "sd2s_outliers_removed_list = []\n",
    "sd_ratios_outliers_removed_list = []\n",
    "out_sd1s_list = []\n",
    "out_sd2s_list = []\n",
    "n_out_list = []\n",
    "for j in range(0, no_channels):\n",
    "    sd1s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    sd2s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    sd_ratios = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    out_sd1s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    out_sd2s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    n_out = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "\n",
    "    i = 0\n",
    "    for signal in denoised_signals[:, j]:\n",
    "        if not np.isnan(signal).all():\n",
    "            sd1, sd2, sd_ratio, out_sd1, out_sd2, out = calculate_poincare_sd(signal, remove_outliers=True)\n",
    "            sd1s[i] = sd1\n",
    "            sd2s[i] = sd2\n",
    "            sd_ratios[i] = sd_ratio\n",
    "            out_sd1s[i] = out_sd1\n",
    "            out_sd2s[i] = out_sd2\n",
    "            n_out[i] = out\n",
    "\n",
    "            i+=1\n",
    "        \n",
    "    sd1s_outliers_removed_list.append(sd1s)\n",
    "    sd2s_outliers_removed_list.append(sd2s)\n",
    "    sd_ratios_outliers_removed_list.append(sd_ratios)\n",
    "    out_sd1s_list.append(out_sd1s)\n",
    "    out_sd2s_list.append(out_sd2s)\n",
    "    n_out_list.append(n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd1s_list = []\n",
    "sd2s_list = []\n",
    "sd_ratios_list = []\n",
    "\n",
    "for j in range(0, no_channels):\n",
    "    sd1s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    sd2s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    sd_ratios = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "\n",
    "    i = 0\n",
    "    for signal in denoised_signals[:, j]:\n",
    "        if not np.isnan(signal).all():\n",
    "            sd1, sd2, sd_ratio = calculate_poincare_sd(signal, remove_outliers=False)\n",
    "            sd1s[i] = sd1\n",
    "            sd2s[i] = sd2\n",
    "            sd_ratios[i] = sd_ratio\n",
    "\n",
    "            i+=1\n",
    "        \n",
    "    sd1s_list.append(sd1s)\n",
    "    sd2s_list.append(sd2s)\n",
    "    sd_ratios_list.append(sd_ratios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #takes a while\n",
    "nk_sd1s_list = []\n",
    "nk_sd2s_list = []\n",
    "nk_sd_ratios_list = []\n",
    "\n",
    "for j in range(0, no_channels):\n",
    "    nk_sd1s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    nk_sd2s = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "    nk_sd_ratios = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "\n",
    "    i = 0\n",
    "    for signal in denoised_signals[:, j]:\n",
    "        if not np.isnan(signal).all():\n",
    "            sd1, sd2, sd_ratio = calculate_poincare_sd(signal, remove_outliers=False, use_nk=True)\n",
    "            nk_sd1s[i] = sd1\n",
    "            nk_sd2s[i] = sd2\n",
    "            nk_sd_ratios[i] = sd_ratio\n",
    "\n",
    "            i+=1\n",
    "        \n",
    "    nk_sd1s_list.append(nk_sd1s)\n",
    "    nk_sd2s_list.append(nk_sd2s)\n",
    "    nk_sd_ratios_list.append(nk_sd_ratios)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shannon_entropy(signal, num_bins=500, use_nk=False):\n",
    "    rr_intervals = get_rri(signal)[0]\n",
    "    \n",
    "    if use_nk:\n",
    "        peaks, info = nk.ecg_peaks(signal, sampling_rate=1000)\n",
    "        nl_indices = nk.hrv_nonlinear(peaks, sampling_rate=1000, show=False)\n",
    "        return nl_indices['HRV_ShanEn']\n",
    "    \n",
    "    # discretize RR intervals into bins and calculate probabilities\n",
    "    hist, bin_edges = np.histogram(rr_intervals, bins=num_bins, density=True)\n",
    "    \n",
    "    probabilities = hist / np.sum(hist)\n",
    "    \n",
    "    # Calculate Shannon Entropy using equation\n",
    "    shannon_entropy = -np.sum(probabilities * np.log2(probabilities + 1e-12))  # adding a small value to avoid log(0)\n",
    "    \n",
    "    return shannon_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have nk option here aswell\n",
    "shannon_ens_list = []\n",
    "\n",
    "for j in range(0, no_channels):\n",
    "    shannon_ens = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "\n",
    "    i=0\n",
    "    for signal in denoised_signals[:, j]:\n",
    "        if not np.isnan(signal).all():\n",
    "            shannon_ens[i] = calculate_shannon_entropy(signal)\n",
    "\n",
    "            i+=1\n",
    "        \n",
    "    shannon_ens_list.append(shannon_ens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_entropy(signal, m=2, r=0.2):\n",
    "    rr_intervals = get_rri(signal)[0]\n",
    "    N = len(rr_intervals)\n",
    "    r *= np.std(rr_intervals)  # tolerance r is usually set as a fraction of the standard deviation\n",
    "    \n",
    "    def _phi(m):\n",
    "        X = np.array([rr_intervals[i:i + m] for i in range(N - m + 1)])\n",
    "        C = np.sum(np.max(np.abs(X[:, None] - X[None, :]), axis=2) <= r, axis=0) - 1\n",
    "        return np.sum(C) / (N - m + 1)\n",
    "    \n",
    "    return -np.log((_phi(m + 1)) / (_phi(m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_ens_list = []\n",
    "# for j in range(0, no_channels):\n",
    "#     samp_ens = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "#     i=0\n",
    "#     for signal in denoised_signals[:, j]:\n",
    "#         if not np.isnan(signal).all():\n",
    "#             samp_en = calculate_sample_entropy(signal)\n",
    "#             if samp_en == np.inf:\n",
    "#                 samp_en = np.nan\n",
    "#             samp_ens[i] = samp_en\n",
    "            \n",
    "#             i+=1\n",
    "#     samp_ens_list.append(samp_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fractal Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating autocorrelation and time delay\n",
    "def autocorrelation(signal):\n",
    "    n = len(signal)\n",
    "    mean = np.mean(signal)\n",
    "    var = np.var(signal)\n",
    "    signal = signal - mean\n",
    "    autocorr = correlate(signal, signal, mode='full')[n-1:] / (var * n) #scipy correlate alot faster\n",
    "    return autocorr\n",
    "\n",
    "def find_time_delay(signal):\n",
    "    autocorr = autocorrelation(signal)\n",
    "    # Find the point where autocorrelation drops to 1/e of its initial value\n",
    "    threshold = 1 / np.exp(1)\n",
    "    tau = np.argmax(autocorr <= threshold) #measured in units of sampling rate (1000 hz)\n",
    "    return tau\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_list = []\n",
    "for j in range(0, no_channels):\n",
    "    tau = np.zeros(shape = len(health_state[nan_indices[j]]), dtype=int)\n",
    "    i = 0\n",
    "    for signal in denoised_signals[:, j]:\n",
    "        if not np.isnan(signal).all():\n",
    "            tau[i] = find_time_delay(signal)\n",
    "            \n",
    "            i+=1\n",
    "    tau_list.append(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding time series\n",
    "def embed_time_series(signal, tau, m):\n",
    "    n = len(signal)\n",
    "    if n < (m - 1) * tau:\n",
    "        raise ValueError(\"Time series is too short for the chosen tau and embedding dimension.\")\n",
    "    embedded = np.array([signal[i : i + (m - 1) * tau + 1 : tau] for i in range(n - (m - 1) * tau)])\n",
    "    return embedded\n",
    "\n",
    "# embedding dimension M\n",
    "M = 4\n",
    "\n",
    "    \n",
    "#creates list of embedded arrays i.e. no_patients * len(embedded) * M\n",
    "\n",
    "embedded_signals_list = []\n",
    "for j in range(0, no_channels):\n",
    "    embedded_signals = []\n",
    "    i = 0\n",
    "    for signal in denoised_signals[:, j]:\n",
    "        if not np.isnan(signal).all():\n",
    "            embedded_signals.append(embed_time_series(signal, tau_list[j][i], M))\n",
    "            \n",
    "            i+=1\n",
    "    embedded_signals_list.append(embedded_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating fractal dimension through box counting\n",
    "def box_counting(data, box_sizes):\n",
    "    counts = []\n",
    "    for size in box_sizes:\n",
    "        count = 0\n",
    "        # creates grid\n",
    "        grid = np.ceil(data / size).astype(int)\n",
    "        #counts unique boxes\n",
    "        unique_boxes = np.unique(grid, axis=0)\n",
    "        count = len(unique_boxes)\n",
    "        counts.append(count)\n",
    "    return counts\n",
    "\n",
    "def fractal_dimension(data, box_sizes):\n",
    "    counts = box_counting(data, box_sizes)\n",
    "    log_box_sizes = np.log(box_sizes)\n",
    "    log_counts = np.log(counts)\n",
    "    \n",
    "    # Perform linear regression to find the slope of the log-log plot\n",
    "    coeffs = np.polyfit(log_box_sizes, log_counts, 1)\n",
    "    return -coeffs[0]  # The fractal dimension is the negative slope\n",
    "\n",
    "#check this whole code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fractal_dim(signal):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    embedded_signal_std = scaler.fit_transform(signal)\n",
    "\n",
    "    # performs SVD and obtains principal components\n",
    "    U, S, VT = svd(embedded_signal_std, full_matrices=False)\n",
    "    PCs = U @ np.diag(S)\n",
    "    \n",
    "    # define box sizes\n",
    "    box_sizes = np.logspace(-1, 1, num=10)  # research/try this out\n",
    "    \n",
    "    # calculate the fractal dimension\n",
    "    fd = fractal_dimension(PCs[:, :3], box_sizes)  # uses the first three principal components, maybe use more??\n",
    "    return fd    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##takes a long time so will leave it out for now \n",
    "# fd_list = []\n",
    "# for j in range(0, no_channels):\n",
    "#     fd = np.zeros(shape = len(health_state[nan_indices[j]]))\n",
    "#     i=0\n",
    "#     for signal in embedded_signals_list[j]:\n",
    "#         fd[i] = find_time_delay(signal)\n",
    "            \n",
    "#         i+=1\n",
    "#     fd_list.append(fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariates\n",
    "- have access to age and gender, age is the only one that makes sense to investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages=np.zeros(no_patients)\n",
    "for i in range(0, no_patients):\n",
    "    ages[i] = allowed_patients.get_patients(i).get_age()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages_list = []\n",
    "for j in range(0, no_channels):\n",
    "    ages_list.append(ages[nan_indices[j]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Parameter Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create dictionary with parameters\n",
    "params = {}\n",
    "\n",
    "#time components\n",
    "params['rr_mean'] = rr_means_list\n",
    "params['rr_std'] = rr_stds_list\n",
    "params['rr_amps'] = rr_amps_list\n",
    "params['RMSSD'] = rr_RMSSD_list\n",
    "params['pNN50'] = rr_pNN50s_list\n",
    "\n",
    "#params['QRS_mean'] = QRS_means_list #contains nan so need to remove those so can use the filtering methods\n",
    "#params['QRS_std'] = QRS_stds_list\n",
    "\n",
    "params['mean'] = means_list\n",
    "params['std'] = stds_list\n",
    "params['skews'] = skews_list\n",
    "params['kurtosis'] = kurtosiss_list\n",
    "\n",
    "#frequency components\n",
    "params['hf'] = hfs_list\n",
    "params['lf'] = lfs_list\n",
    "params['power_ratio'] = ratios_list\n",
    "params['total_power'] = tps_list\n",
    "\n",
    "#nonlinear components\n",
    "params['shannon_en'] = shannon_ens_list\n",
    "#params['sample_en'] = samp_ens_list need to do imputing if want to include it, due to some nan values\n",
    "params['sd_ratio'] = sd_ratios_list\n",
    "params['nk_sd_ratio'] = nk_sd_ratios_list\n",
    "params['sd_ratio_outliers_removed'] = sd_ratios_outliers_removed_list\n",
    "#params['fd'] = fd_list\n",
    "\n",
    "#covariates\n",
    "params['age'] = ages_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
